{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS - Hypotension Prediction from Vital Signs - Part 1\n",
    "\n",
    "Physiologic Time Series Data\n",
    "\n",
    "History\n",
    "- 11/28/2022 modified for 2022 BIDS CDA class\n",
    "- 12/27/2023 updated for 2023 BIDS CDA class\n",
    "\n",
    "Objectives\n",
    "- Physiologic Time Series Data\n",
    "   - Sources\n",
    "   - Meaning\n",
    "   - Numerics vs Waveforms\n",
    "   \n",
    "References\n",
    " - Yoon Critical Care 2020;24:661\n",
    " - Lee Biomedical Engineering Online 2010;9:62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def sub_ans(team,question_num,answer):\n",
    "    url='https://bids-class.azurewebsites.net/submit-answer'\n",
    "    data={'class':'cda',\n",
    "         'module':'cda_12',\n",
    "         'team':team,\n",
    "         'question_num':question_num,\n",
    "         'answer_num':answer}\n",
    "    x=requests.post(url,data=data)\n",
    "    if x.status_code==200:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# work around, old versions of python missing new string method in python 3.9\n",
    "def removesuffix(string, suffix):\n",
    "    assert string.endswith(suffix)\n",
    "    return string[:-len(suffix)]\n",
    "assert removesuffix(\"foo.hea\", \".hea\") == \"foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.width = 200\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.options.display.max_rows = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Physiologic Time Series Data\n",
    "\n",
    "We're using data from the MIMIC-III Waveform Database Matched Subset.\n",
    "\n",
    "https://physionet.org/content/mimic3wdb-matched/1.0/\n",
    "\n",
    "It contains:\n",
    "- waveforms: continuously sampled signals at 60-250 Hz, such as EKG\n",
    "- numerics: numeric values computed from waveforms at 0.16-1 Hz, such as HR computed from an EKG.\n",
    "\n",
    "MIT/PhysioNet defines a format and provides a library for working with these signals.  Before importing it, use pip to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pull down some data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = [\n",
    "    'p00/p000020/3544749n.dat',\n",
    "\n",
    "    'p00/p000079/3842928n.dat',\n",
    "    'p00/p000079/3887555n.dat',\n",
    "\n",
    "    'p00/p000109/3002540n.dat',\n",
    "    'p00/p000123/3348632n.dat',\n",
    "    'p00/p000123/3485814n.dat',\n",
    "\n",
    "    'p00/p000124/3255538n.dat',\n",
    "    'p00/p000124/3526846n.dat',\n",
    "    'p00/p000124/3644535n.dat',\n",
    "    'p00/p000124/3776642n.dat',\n",
    "    'p00/p000124/3807277n.dat',\n",
    "    'p00/p000124/3931528n.dat',\n",
    "    'p00/p000124/3972293n.dat',\n",
    "    'p00/p000124/3991520n.dat',\n",
    "]\n",
    "\n",
    "num_hdrs = [\n",
    "    'p00/p000020/p000020-2183-04-28-17-47n.hea',\n",
    "    'p00/p000079/p000079-2175-09-26-01-25n.hea',\n",
    "    'p00/p000079/p000079-2175-09-26-12-28n.hea',\n",
    "    'p00/p000123/p000123-2161-10-29-03-01n.hea',\n",
    "    #'p00/p000123/p000123-2161-10-30-12-58n.hea',\n",
    "    'p00/p000124/p000124-2160-07-05-18-27n.hea',\n",
    "    'p00/p000124/p000124-2160-07-05-21-51n.hea',\n",
    "    'p00/p000124/p000124-2160-07-06-15-23n.hea',\n",
    "    #'p00/p000124/p000124-2160-07-11-12-21n.hea',\n",
    "    #'p00/p000124/p000124-2160-07-14-01-20n.hea',\n",
    "    #'p00/p000124/p000124-2166-01-09-13-03n.hea',\n",
    "    #'p00/p000124/p000124-2166-01-11-23-28n.hea',\n",
    "    #'p00/p000124/p000124-2166-01-23-10-43n.hea',\n",
    "]\n",
    "    \n",
    "wave_data = [\n",
    "    'p00/p000020/3544749_0001.dat',\n",
    "    'p00/p000020/3544749_0001.hea',\n",
    "    'p00/p000020/3544749_0002.dat',\n",
    "    'p00/p000020/3544749_0002.hea',\n",
    "    'p00/p000020/3544749_0003.dat',\n",
    "    'p00/p000020/3544749_0003.hea',\n",
    "    'p00/p000020/3544749_0004.dat',\n",
    "    'p00/p000020/3544749_0004.hea',\n",
    "    'p00/p000020/3544749_0005.dat',\n",
    "    'p00/p000020/3544749_0005.hea',\n",
    "    'p00/p000020/3544749_0006.dat',\n",
    "    'p00/p000020/3544749_0006.hea',\n",
    "    'p00/p000020/3544749_0007.dat',\n",
    "    'p00/p000020/3544749_0007.hea',\n",
    "    'p00/p000020/3544749_0008.dat',\n",
    "    'p00/p000020/3544749_0008.hea',\n",
    "    'p00/p000020/3544749_layout.hea',\n",
    "]\n",
    "    \n",
    "wave_hdrs = [\n",
    "    'p00/p000020/p000020-2183-04-28-17-47.hea',\n",
    "]\n",
    "    \n",
    "dl_dir = '.'\n",
    "\n",
    "#\n",
    "# Only have to download once.\n",
    "#\n",
    "# However, wfdb.dl_files is smart and only re-downloads if file is missing\n",
    "# or has different size ...\n",
    "#\n",
    "wfdb.dl_files('mimic3wdb-matched', dl_dir, num_data)\n",
    "wfdb.dl_files('mimic3wdb-matched', dl_dir, num_hdrs)\n",
    "\n",
    "wfdb.dl_files('mimic3wdb-matched', dl_dir, wave_data)\n",
    "wfdb.dl_files('mimic3wdb-matched', dl_dir, wave_hdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load numeric data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recs = [wfdb.rdrecord(removesuffix(_, '.hea')) for _ in num_hdrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = num_recs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `rec`?  (Hint, use the `?`)\n",
    "\n",
    "```\n",
    "rec?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### Q1\n",
    "\n",
    "How many signals are in this record?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Find number of signals\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "#\n",
    "# Approach #1: use n_sig\n",
    "#\n",
    "rec.n_sig\n",
    "\n",
    "#\n",
    "# Approach #2: query shape of signal array `p_signal`\n",
    "#\n",
    "rec.p_signal.shape\n",
    "\n",
    "#\n",
    "# Or more exactly, find the number of columns:\n",
    "#\n",
    "rec.p_signal.shape[1]\n",
    "\n",
    "#\n",
    "# Approach #3: length of signal names\n",
    "#\n",
    "len(rec.sig_name)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first rec into a Pandas DataFrame named \n",
    "- `rec.p_signal` is a numpy array\n",
    "- `rec.sig_name` defines the signal (aka column) names\n",
    "\n",
    "We'll also limit to just the HR (EKG), Pulse (HR from Arterial line), RESP, and Arterial BP signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a function to convert record into dataframe\n",
    "#\n",
    "def create_df_from_record(rec):\n",
    "    \"\"\"\n",
    "    Create a pandas datafrom from a wfdb record\n",
    "    \n",
    "    Args\n",
    "     - rec - wfdb record\n",
    "     \n",
    "    Returns a pandas dataframe with one column for each signal, \n",
    "      plus a 'dt' column with the datetime of each sample\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(rec.p_signal, columns=rec.sig_name)\n",
    "    \n",
    "    #\n",
    "    # Find timestamp at each sample:\n",
    "    #   timestamp = base_date + (sample number / frequency)\n",
    "    #\n",
    "    if np.isclose(rec.fs, 1/60):\n",
    "        # avoid floating-point rounding for q1m data\n",
    "        delta = pd.Series(range(df.shape[0])) * 60\n",
    "    else:\n",
    "        delta = pd.Series(range(df.shape[0])) / rec.fs\n",
    "        \n",
    "    df['dt'] = rec.base_datetime + pd.to_timedelta(delta, unit='seconds')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?create_df_from_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create record of numerics\n",
    "#\n",
    "df_raw = create_df_from_record(rec)\n",
    "df_raw = df_raw[['dt', 'HR', 'PULSE', 'RESP', 'ABPSys', 'ABPDias', 'ABPMean', 'SpO2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### Q2\n",
    "\n",
    "How many HR values are zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Number of zero HR values\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "(df_raw.HR == 0).sum()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### Q3\n",
    "\n",
    "How many PULSE values are zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Number of zero PULSE values\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "(df_raw.PULSE == 0).sum()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think they are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate / Multivariate plots and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the 'PULSE' and 'HR' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.PULSE.hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.HR.hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a scatter plot of HR vs PULSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.plot.scatter('HR', 'PULSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a scatter plot of HR vs PULSE\n",
    "- make dots smaller (marker='.', s=16)\n",
    "- use an alpha < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.plot.scatter('HR', 'PULSE', marker='.', s=16, alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think they are not more linearly related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4\n",
    "\n",
    "Compute the standard correlation coefficient between `HR` and `PULSE` using `pd.corr`.  Round result to 2 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: `HR` vs `PULSE` correlation\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "#\n",
    "# create projection with just columns of intereste\n",
    "#\n",
    "df_raw[['HR', 'PULSE']].corr()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-series plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to plot the variables over time (we'll use this multiple times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df, mm=None, imm=None):\n",
    "    \"\"\"\n",
    "    Plot numeric physiologic data\n",
    "    \n",
    "    Args:\n",
    "     - df - dataframe with HR, PULSE, RESP, ABP{Sys,Dias,Mean}, and SpO2\n",
    "            columns\n",
    "     - imm - optional min/max of X axis (by index)\n",
    "     - mm  - optionsl min/max of X axis (by time)\n",
    "    \"\"\"\n",
    "    n = 4\n",
    "    # create a figure with 4 subplots\n",
    "    fig, ax = plt.subplots(n, 1)\n",
    "    \n",
    "    # set height to something reasonable\n",
    "    fig.set_figheight(n*2.5)\n",
    "    fig.set_figwidth(12)\n",
    "\n",
    "    # Plot HR/PUlse in first\n",
    "    ax[0].set_ylabel('HR')\n",
    "    ax[0].plot(df.dt, df.HR, label='HR')\n",
    "    ax[0].plot(df.dt, df.PULSE, label='Pulse')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # RESP in second\n",
    "    ax[1].set_ylabel('RESP')\n",
    "    ax[1].plot(df.dt, df.RESP)\n",
    "\n",
    "    # Artieral BP in third\n",
    "    ax[2].set_ylabel('ABP')\n",
    "    ax[2].plot(df.dt, df.ABPSys, label='Sys')\n",
    "    ax[2].plot(df.dt, df.ABPDias, label='Dias')\n",
    "    ax[2].plot(df.dt, df.ABPMean, label='mABP')\n",
    "    ax[2].legend()\n",
    "    \n",
    "    # SpO2 in final\n",
    "    ax[3].set_ylabel('SpO2')\n",
    "    ax[3].plot(df.dt, df.SpO2)\n",
    "\n",
    "    # if user doesn't specify the min/max value for the x-axis,\n",
    "    # find the min/max across all subplots\n",
    "    if imm is not None:\n",
    "        assert mm is None, \"Can't use both mm and imm opts\"\n",
    "        mm = [df.loc[mm[0], 'dt'], df.loc[imm[0], 'dt'] ]\n",
    "    elif mm is None:\n",
    "        mm = ax[0].get_xlim()\n",
    "\n",
    "        for i in range(1, len(ax)):\n",
    "            x0, x1 = ax[i].get_xlim()\n",
    "            if x0 < mm[0]: mm[0] = x0\n",
    "            if x1 > mm[1]: mm[1] = x1\n",
    "            \n",
    "    # sync all subplots\n",
    "    for ent in ax:\n",
    "        ent.set_xlim(*mm)\n",
    "        \n",
    "    # return the fig and ax in case we want to\n",
    "    # embellish them further\n",
    "    return fig, ax\n",
    "        \n",
    "plot_df(df_raw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this change your idea why PULSE and HR have different numbers of zero values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief look at Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load waveform records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_recs = [wfdb.rdrecord(removesuffix(_, '.hea')) for _ in wave_hdrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe (using `create_df_from_record`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrec = wave_recs[0]\n",
    "df_wave = create_df_from_record(wrec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine some rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wave.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip ahead to index 10000\n",
    "- first rows are empty (np.nan)\n",
    "- after that are some test patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wave[10000:10005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5\n",
    "\n",
    "How many waveform signals are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to plot waveforms alongside some numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dfn, dfw, sigs, mm=None):    \n",
    "    \"\"\"\n",
    "    Plot numeric/waveform physiologic data\n",
    "    \n",
    "    Args:\n",
    "     - df - dataframe with HR, PULSE, RESP, ABP{Sys,Dias,Mean}, and SpO2\n",
    "            columns\n",
    "     - sigs -  list of signal types to plot (choices include abp, ekg, hr)\n",
    "     - mm  - optional min/max of X axis (by time)\n",
    "    \"\"\"\n",
    "    assert mm is not None and len(mm) == 2\n",
    "    \n",
    "    if type(mm[0]) is str: mm[0] = parser.parse(mm[0])\n",
    "    if type(mm[1]) is str: mm[1] = parser.parse(mm[1])\n",
    "    \n",
    "    n = len(sigs)\n",
    "    fig, raw_ax = plt.subplots(n, 1)\n",
    "    if n == 1: raw_ax = [raw_ax]\n",
    "    \n",
    "    # set height to something reasonable\n",
    "    fig.set_figheight(n*2.5)\n",
    "    fig.set_figwidth(12)\n",
    "    \n",
    "    ax = {sig: raw_ax[i] for i, sig in enumerate(sigs)}\n",
    "    #ax = {}\n",
    "    #for i, sig in enumerate(sigs):\n",
    "    #    ax[sig] = raw_ax[i]\n",
    "    \n",
    "    #\n",
    "    # limit plot data to near min/max window\n",
    "    #  - for numeric, extend window by a minute on both side, since\n",
    "    #    data only available every minute\n",
    "    tdfn = dfn[(dfn.dt >= mm[0] - pd.Timedelta(seconds=120)) & \n",
    "               (dfn.dt <= mm[1] + pd.Timedelta(seconds=120))]\n",
    "    tdfw = dfw[(dfw.dt >= mm[0]) & (dfw.dt <= mm[1])]\n",
    "    \n",
    "    if 'abp' in ax:\n",
    "        ax['abp'].set_ylabel('ABP')\n",
    "        ax['abp'].plot(tdfn.dt, tdfn.ABPSys,  drawstyle='steps-post', label='Sys')\n",
    "        ax['abp'].plot(tdfn.dt, tdfn.ABPDias, drawstyle='steps-post', label='Dias')\n",
    "        ax['abp'].plot(tdfn.dt, tdfn.ABPMean, drawstyle='steps-post', label='mABP')\n",
    "\n",
    "        ax['abp'].plot(tdfw.dt, tdfw.ABP, label='ABP', alpha=0.3)\n",
    "\n",
    "        ax['abp'].legend()\n",
    "        \n",
    "    if 'ekg' in ax:\n",
    "        ax['ekg'].set_ylabel('EKG')\n",
    "        ax['ekg'].plot(tdfw.dt, tdfw.II, label='II')\n",
    "        #ax['ekg'].plot(tdfw.dt, tdfw.AVF, label='AVF')\n",
    "\n",
    "        ax['ekg'].legend()\n",
    "\n",
    "    if 'hr' in ax:\n",
    "        ax['hr'].set_ylabel('HR')\n",
    "        ax['hr'].plot(tdfn.dt, tdfn.HR, drawstyle='steps-post', label='HR', )\n",
    "        #ax['hr'].plot(tdfn.dt, tdfn.PULSE, drawstyle='steps-post', label='PULSE')\n",
    "        ax['hr'].legend()\n",
    "\n",
    "    if mm is not None:\n",
    "        for axi in ax.values():\n",
    "            axi.set_xlim(*mm)\n",
    "        \n",
    "    # return the fig and ax in case we want to\n",
    "    # embellish them further\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Minute\n",
    "\n",
    "Blood pressure variation with respiratory pattern.\n",
    "\n",
    "Illustrates the different levels of information available between q1m data and 125 Hz data.\n",
    "\n",
    "Also see that EKG is affected (however this could be related to motion of the sensors relative to the heart and not intrinsic to the cardiac elecrical activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(df_raw, df_wave, sigs=['ekg', 'abp'],\n",
    "         mm = ['2183-04-28 17:49:00',\n",
    "               '2183-04-28 17:50:00']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoom out - plot 15 minutes\n",
    "\n",
    "Notice how the numeric values of BP (Sys, Dias, mABP) correspond to the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(df_raw, df_wave, sigs=['ekg', 'abp'],\n",
    "         mm = ['2183-04-28 17:49:00',\n",
    "               '2183-04-28 18:04:00']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoom in - plot 10 seconds\n",
    "\n",
    "Finer view of EKG shows pacing spikes.  This is why heart rate is so constant during first few days.\n",
    "\n",
    "One spike occurred on top of intrinsic atrial contraction, leading to ineffective ventricular contraction.\n",
    "\n",
    "Illustrates challenge with the meaning of \"HR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(df_raw, df_wave, sigs = ['ekg', 'abp'],\n",
    "         mm = ['2183-04-28 17:49:20',\n",
    "               '2183-04-28 17:49:30']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 seconds later in record (after pacing stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both(df_raw, df_wave, sigs=['ekg', 'hr'],\n",
    "         mm = ['2183-04-29 12:01:20',\n",
    "               '2183-04-29 12:01:30']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot 30 minutes of EKG waveform & HR numerics (after pacing stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_both(df_raw, df_wave, sigs=['ekg', 'hr'],\n",
    "         mm = ['2183-04-29 12:00:00',\n",
    "               '2183-04-29 12:30:00']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing -- Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physiologic time series data are typically recorded automatically from the patient, with either minimal or no human review of values.  This has the benefit of reducing *bias* -- the vitals are continuously sampled regardless of the patient's status, in comparison to other measures such nurse documented vitals which may be sampled more/less frequently based on perceived patient acuity, or laboratory values that may only be sampled when there is clinical suspicion of abnormality.\n",
    "\n",
    "This also has the downside that artefacts, such as a disconnected sensor or an external event that impedes the sensor, are left within the record.\n",
    "\n",
    "**Outlier removal** is the process of removing these artifacts.  Typically there are 2-3 components:\n",
    "- removal of physiologically implausible values, e.g. HR of a 1000, a blood pressure where the mean value is higher than the systolic value, etc.\n",
    "- removal of statistically improbable values, e.g. HR's greater than 2-3 standard deviations from the mean, where the mean could be for the study population.\n",
    "  - for a normally distributed variable (is your variable normally distributed?): values greater than 2-3 standard deviations from the mean\n",
    "  - for other distributions, consider quantiles: < 0.5% or > 99.5%\n",
    "- noise filter, such as a median filter\n",
    "\n",
    "However, sometimes physiology of interest is improbable:\n",
    "- During cardiac arrest, HR will be zero (measured from pulse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physiologic Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** create a function named `remove_outliers` that:\n",
    "\n",
    "- Takes a dataframe with numeric PTSD (such as `df_raw`)\n",
    "\n",
    "- Applies the following physiologic plausible ranges used by Yoon *et al*:\n",
    "\n",
    "    - HR [20-220] (use for Pulse too)\n",
    "    - RR [1-60]\n",
    "    - BP [10-250]\n",
    "       - apply to each component: systolic, diastolic, mean\n",
    "       - check that systolic >= mean >= diastolic\n",
    "       - if any component is invalid => assume all invalid\n",
    "    - SpO2 [10-100] (*)\n",
    "\n",
    "- Replaces outliers with `np.nan`.\n",
    "\n",
    "- Print the number of HR, Pulse, and RESP values that are invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # create masks:\n",
    "    m_invalid_hr    = ... # your solution here\n",
    "    m_invalid_pulse = ...\n",
    "    m_invalid_resp  = ...\n",
    "    m_invalid_abp   = ...\n",
    "    m_invalid_spo2  = ...\n",
    "    \n",
    "\n",
    "    # set outliers to np.nan\n",
    "    df.loc[m_invalid_hr, 'HR'] = np.nan\n",
    "    df.loc[m_invalid_pulse, 'PULSE'] = np.nan\n",
    "\n",
    "    df.loc[m_invalid_resp, 'RESP']   = np.nan\n",
    "\n",
    "    df.loc[m_invalid_abp, 'ABPSys']  = np.nan\n",
    "    df.loc[m_invalid_abp, 'ABPDias'] = np.nan\n",
    "    df.loc[m_invalid_abp, 'ABPMean'] = np.nan\n",
    "\n",
    "    df.loc[m_invalid_spo2, 'SpO2'] = np.nan\n",
    "    \n",
    "    # Print how many values are invalid\n",
    "    print(f'num invalid HR    {np.sum(m_invalid_hr)}')\n",
    "    print(f'num invalid Pulse {np.sum(m_invalid_pulse)}')\n",
    "    print(f'num invalid Resp  {np.sum(m_invalid_resp)}')\n",
    "    print(f'num invalid ABP   {np.sum(m_invalid_abp)}')\n",
    "    print(f'num invalid SpO2  {np.sum(m_invalid_spo2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Outlier removal: physiologically implausible values\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "def remove_outliers(df):\n",
    "    # create masks:\n",
    "    m_invalid_hr    = (df.HR < 10) | (df.HR > 220)\n",
    "    m_invalid_pulse = (df.PULSE < 10) | (df.PULSE > 220)\n",
    "    m_invalid_resp  = (df.RESP  < 1)  | (df.RESP  > 60)\n",
    "    m_invalid_abp   = ((df.ABPSys < 10)  | (df.ABPSys > 250) |\n",
    "                       (df.ABPDias < 10) | (df.ABPDias > 250) |\n",
    "                       (df.ABPMean < 10)  | (df.ABPMean > 250) |\n",
    "                       (df.ABPMean > df.ABPSys) | (df.ABPMean < df.ABPDias))\n",
    "    m_invalid_spo2  = (df.SpO2 < 10) | (df.SpO2 > 100)\n",
    "    \n",
    "\n",
    "    # set outliers to np.nan\n",
    "    df.loc[m_invalid_hr, 'HR'] = np.nan\n",
    "    df.loc[m_invalid_pulse, 'PULSE'] = np.nan\n",
    "\n",
    "    df.loc[m_invalid_resp, 'RESP']   = np.nan\n",
    "\n",
    "    df.loc[m_invalid_abp, 'ABPSys']  = np.nan\n",
    "    df.loc[m_invalid_abp, 'ABPDias'] = np.nan\n",
    "    df.loc[m_invalid_abp, 'ABPMean'] = np.nan\n",
    "\n",
    "    df.loc[m_invalid_spo2, 'SpO2'] = np.nan\n",
    "    \n",
    "    # Print how many values are invalid\n",
    "    print(f'num invalid HR    {np.sum(m_invalid_hr)}')\n",
    "    print(f'num invalid Pulse {np.sum(m_invalid_pulse)}')\n",
    "    print(f'num invalid Resp  {np.sum(m_invalid_resp)}')\n",
    "    print(f'num invalid ABP   {np.sum(m_invalid_abp)}')\n",
    "    print(f'num invalid SpO2  {np.sum(m_invalid_spo2)}')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to our data:\n",
    "\n",
    "(note: we'll make a copy of the dataframe first, so that we can run it multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = df_raw.copy()\n",
    "remove_outliers(df_clean_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6\n",
    "\n",
    "How many ABP values were invalid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,6,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df(df_clean_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Outlier Rejection\n",
    "\n",
    "After implausible/impossible values are removed, it is common to remove statistical outliers\n",
    "- either based on population distribution,\n",
    "- or local distribution for the patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** write a function `remove_outliers_stat` that\n",
    "- takes arguments\n",
    "   - numeric dataframe\n",
    "   - threshold value such that outlier with quantile < threshold or > threshold are removed\n",
    "- computes cutoff values `minv` and `maxv` from threshold using `pd.Series.quantile`\n",
    "  ([docs](https://pandas.pydata.org/docs/reference/api/pandas.Series.quantile.html))\n",
    "- sets values outside cutoffs to `np.nan`\n",
    "- prints the number of outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_stat(df, thresh):\n",
    "    for col in df.columns:\n",
    "        if col == 'dt': continue\n",
    "        ...\n",
    "        # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Outlier removal: statiscially implausible values\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "def remove_outliers_stat(df, thresh):\n",
    "    for col in df.columns:\n",
    "        if col == 'dt': continue\n",
    "        minv, maxv = df[col].quantile([thresh, 1-thresh])\n",
    "        m = (df[col] < minv) | (df[col] > maxv)\n",
    "        df.loc[m, col] = np.nan\n",
    "        print(f' - {col}: {m.sum()} outliers removed')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process our data ith thresh = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_2 = df_clean_1.copy()\n",
    "remove_outliers_stat(df_clean_2, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q7\n",
    "\n",
    "How many ABP statistical outliers were removed (for ABPSys)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,7,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation\n",
    "\n",
    "Machine learning algorithms typically assume features are available at the time of prediction.  It is necessary to impute missing values, both missing in the original dataset, and missing because of outlier removal.\n",
    "\n",
    "Missing value imputation can range from simple to complex\n",
    "- population mean value imputation\n",
    "- last value carried foward (aka foward fill)\n",
    "- summary of recent values carried forward (eg forward fill local mean)\n",
    "- missingness as a feature (impute + missing flag)\n",
    "- linear regression (be careful to avoid non-causality)\n",
    "- multiple imputation (create set of imputations statistically similar to real values)\n",
    "- fit non-mising data gaussian processes (exploit correlation across variables and time)\n",
    "\n",
    "Resampling and downsampling can affect missing value imputation.\n",
    "- Background\n",
    "  - longer time series can provide more information\n",
    "  - however, more values requires larger models (with more parameters) and larger training sets\n",
    "  - tradeoff: use statistical summaries of time series\n",
    "     - mean, standard deviation, min, max, etc\n",
    "     - break into regions (remote, recent, etc)\n",
    "- Statistical summaries essentially downsample time-series\n",
    "  - Eg Average 60 minutes of q1m HR\n",
    "  - Approach: ignore missing values\n",
    "  - Summaries somewhat resilient to small missingingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Yoon *et al*'s imputation strategy:\n",
    "- impute missing value with moving average of previous 3 values\n",
    "- IF missing data time period < 10 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise:** use pandas rolling window calculations (`rolling()`) to compute rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.rolling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by computing rolling average of synthetic vector of last three samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tmp():\n",
    "    tmp = pd.DataFrame(np.zeros(shape=(20,1), dtype=float), columns=['signal'])\n",
    "    tmp.loc[0, 'signal'] = 5\n",
    "    tmp.loc[3:4, 'signal'] = np.nan # Small gap (***) notice that pandas ranges are different than standard python\n",
    "    tmp.loc[6, 'signal'] = 2\n",
    "    tmp.loc[8:17, 'signal'] = np.nan # big gap\n",
    "    return tmp\n",
    "\n",
    "tmp = create_tmp()\n",
    "\n",
    "# (***) Notice that pandas slices are different than standard python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `valid` column: 'True' when signal is not missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['valid'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: label valid time samples\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['valid'] = tmp.signal.notnull()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q7\n",
    "\n",
    "How many valid values are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name, 7, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute rolling average of last three minutes (*) (window size = 3):\n",
    "\n",
    "(*) computing average of previous 3 non-missing values is a little harder\n",
    "\n",
    "Hint, look at `pd.Series.rolling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['avg3']  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: rolling average of 3 previous values\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['avg3']  = tmp.signal.rolling(3).mean()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill forward with `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['avg3_ffill'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: Forward fill\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['avg3_ffill'] = tmp.avg3.fillna(method='ffill')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label start/stop of missing values runs with `run_ss`\n",
    "  <br>(hint look at pandas [shift](https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html) )\n",
    "- cumsum start/stop to label `group`s (noting that groups will alternate between valid and invalid)\n",
    "  <br>(hint look at pandas [cumsum](https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['run_ss'] = ...\n",
    "tmp['group']  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: label satart stop, define groups\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['run_ss'] = tmp.valid != tmp.valid.shift()\n",
    "tmp['group']  = tmp.run_ss.cumsum()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine intermediate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the size of each group, set mask_10 if group_size less than 10.\n",
    "\n",
    "Hints:\n",
    "- consider [groupyby](https://pandas.pydata.org/docs/reference/api/pandas.Series.groupby.html) on group to get size\n",
    "- then [map](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html) group to group size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['group_size'] = ...\n",
    "tmp['mask_10']    = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: group size\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['group_size'] = tmp.group.map(tmp.valid.groupby(tmp.group).count())\n",
    "tmp['mask_10']    = tmp.group_size < 10\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q8\n",
    "\n",
    "What is the largest group size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name, 8, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# What is `tmp.group.map(tmp.valid.groupby(tmp.group).count())` ?\n",
    "#\n",
    "# Break it down:\n",
    "# OUTER: tmp.group.map(INNER)\n",
    "# INNER: [tmp.valid].groupby([tmp.group]).count()\n",
    "#        [series]   .groupby([series])   .count()\n",
    "#   => Series.Groupby\n",
    "#\n",
    "#   First series (tmp.valid) is grouped by second series (tmp.group) and count function\n",
    "#   is applied.\n",
    "#\n",
    "#   Here this returns a series where index = group and value equals group size.\n",
    "#   (note we could pick any column for tmp.valid ... count() only cares about size).\n",
    "#\n",
    "# OUTER: tmp.group.map(INNER)\n",
    "#   Transform group into group size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Alternate formulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate #1: use SeriesGroupby.transform with lambda\n",
    "#\n",
    "#   tmp['group_size'] = tmp.groupby('group')['run_ss'].transform(lambda _: _.shape[0]) \n",
    "#\n",
    "# (*) SeriesGroupby.transform returns a series with the same indices.\n",
    "#   - The function must either return a series of the same shape, or\n",
    "#     a value that can broadcast to the group.\n",
    "#   - We take the shape of `run_ss`, but any column would work.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate #2: use DataFrameGroupby.transfrom with count\n",
    "#\n",
    "#   tmp['group_size'] =  tmp.groupby('group')['run_ss'].transform('count')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is faster? ... Benchmark it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup: create a long signal\n",
    "#tmp_signal = []\n",
    "#for r in num_recs:\n",
    "#    tmp_signal.extend\n",
    "\n",
    "tmp1 = pd.DataFrame({'signal': np.concatenate([_.p_signal[:, 0] for _ in num_recs], axis=0)})\n",
    "print(f'tmp1 shape: {tmp1.shape}')\n",
    "tmp1['valid'] = tmp1.signal.notnull()\n",
    "tmp1['run_ss'] = tmp1.valid != tmp1.valid.shift()\n",
    "tmp1['group']  = tmp1.run_ss.cumsum()\n",
    "tmp1['group_size'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tmp1['group_size'] = tmp1.group.map(tmp1.valid.groupby(tmp1.group).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tmp1['group_size'] = tmp1.groupby('group')['run_ss'].transform(lambda _: _.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tmp1['group_size'] = tmp1.groupby('group')['run_ss'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A snazzy way to benchmark the `perfplot` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Unfortunately, this doesn't work on default crunchr container,\n",
    "# even if perfplot is insstalled\n",
    "#\n",
    "\n",
    "import perfplot\n",
    "\n",
    "np.random.seed(0)\n",
    "df_ = pd.DataFrame(np.random.randn(5, 1000))\n",
    "\n",
    "perfplot.show(\n",
    "    setup=lambda n: pd.concat([tmp1] * n, ignore_index=True),\n",
    "    kernels=[\n",
    "        lambda df:  df.group.map(df.valid.groupby(df.group).count()),\n",
    "        lambda df:  df.groupby('group')['run_ss'].transform(lambda _: _.shape[0]),\n",
    "        lambda df:  df.groupby('group')['run_ss'].transform('count')\n",
    "    ],\n",
    "    labels=['map(group)', 'groupby-lambda', 'groupby-count'],\n",
    "    n_range=[2**k for k in range(0, 10)],\n",
    "    xlabel='N (* len(df))',\n",
    "    logx=True,\n",
    "    logy=True,\n",
    "    flops=lambda n: n)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mask that is true when a value is missing (~valid) and can be imputed (group_size < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['mask_impute'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: determine mask_impute\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['mask_impute'] = ~tmp.valid & (tmp.group_size < 10)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q9\n",
    "\n",
    "How many values are imputed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name, 9, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `new_signal` to imputed value when `mask_impute` True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['new_signal'] = tmp.signal\n",
    "tmp.loc[tmp.mask_impute, 'new_signal'] = tmp.loc[tmp.mask_impute, 'avg3_ffill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: apply imputation to create new_signal\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "tmp['new_signal'] = tmp.signal\n",
    "tmp.loc[tmp.mask_impute, 'new_signal'] = tmp.loc[tmp.mask_impute, 'avg3_ffill']\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a function `impute` that does all this to impute a column in-place.\n",
    "\n",
    "We will call this for each columna from `impute_all` function (given).\n",
    "\n",
    "(Hint: use pandas series as variables to avoid putting temporary values into the main dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(signal):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     - signal -- series to perform imputation on\n",
    "    \"\"\"\n",
    "    valid        = ...\n",
    "    avg3         = ...\n",
    "    avg3_ffill   = ...\n",
    "    run_ss       = ...\n",
    "    group        = ...\n",
    "    group_size   = ...\n",
    "    m_impute     = ...\n",
    "    \n",
    "    result = np.where(m_impute, avg3_ffill, signal)\n",
    "    print(f'impute {m_impute.sum()}')\n",
    "    return result\n",
    "    \n",
    "def impute_all(df):\n",
    "    for var in df.columns:\n",
    "        if var == 'dt': continue\n",
    "        print(f'{var}: ', end='')\n",
    "        impute(df[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "  <div class=\"alert alert-block alert-info\">\n",
    "    Solution: impute function\n",
    "  </div>\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "def impute(signal):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     - signal -- series to perform imputation on\n",
    "    \"\"\"\n",
    "    valid        = signal.notnull()\n",
    "    avg3         = signal.rolling(3).mean()\n",
    "    avg3_ffill   = avg3.fillna(method='ffill')\n",
    "    run_ss       = valid != valid.shift()\n",
    "    group        = run_ss.cumsum()\n",
    "    group_size   = group.map(valid.groupby(group).count())\n",
    "    m_impute     = ~valid & (group_size < 10)\n",
    "                                     \n",
    "    result = np.where(m_impute, avg3_ffill, signal)\n",
    "    print(f'impute {m_impute.sum()}')\n",
    "    return result \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `impute` on a fresh temp dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = create_tmp()\n",
    "tmp['imputed'] = impute(tmp['signal'])\n",
    "tmp.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `impute_all` on our signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute = df_clean_2.copy()\n",
    "impute_all(df_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q10\n",
    "\n",
    "How many ABPSys values were imputed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ans(team_name,10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Hypotension Events - For next week ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform supervised learning, we need to label the outcome for training.\n",
    "\n",
    "We'll use Yoon *et al*'s Hypotension (HoTN) definition:\n",
    " - SBP<90 and MAP<60 for >=5 min in 10 min interval\n",
    " - combine events within 2 minutes (*1)\n",
    "\n",
    "Target labels\n",
    " - assign positive label in 15 minutes prior to event (*2)\n",
    "\n",
    "Ambiguity:\n",
    " - (*1) does interval end at 10 minutes, or when BP normalizes?\n",
    " - (*2) this effetively merges HoTN events within 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: create a function to label hypotension.\n",
    "\n",
    "Strategy\n",
    "- identify individual minutes of hypotension\n",
    "- rolling sum over last 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
